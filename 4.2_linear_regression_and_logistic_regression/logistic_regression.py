import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from mpl_toolkits.mplot3d import Axes3D  # need to keep this for 3D plot

# from mpl_toolkits.mplot3d import Axes3D  # need to keep this for 3D plot


def get_data(fname='classification.txt'):
    """
        given a file name, return a pandas data frame
        assume separated by comma
    """
    raw_data = np.array(pd.read_csv(fname, sep=",", header=None))
    X, y = raw_data[:, :3], raw_data[:, 4]
    return X, y


class LogsiticRegression:
    def __init__(self, X, y, max_iter=7000, learning_rate=0.5):
        self.X = X
        self.y = y
        self.max_iter = max_iter
        self.learning_rate = learning_rate
        self.weights = np.ones((4,))

    def fit(self):
        x_zero = np.ones((self.X.shape[0], 1))  # bias value add to X.
        self.X = np.concatenate((x_zero, X), axis=1)  # init X0 as 1
        for _ in range(self.max_iter):
            score = np.multiply((np.dot(self.X, self.weights.T)), -self.y)
            predictions = self.sigmoid(score)
            errors = predictions * self.X.T * (-self.y)
            gradient = np.average(errors, axis=1)  # same as np.sum(temp, axis=1)/len(self.X)
            self.weights -= self.learning_rate * gradient

        return self.weights

    def sigmoid(self, score):
        return 1 / (1 + np.exp(-score))

    def verify(self):
        """
            get the accuracy of the algorithm
        """
        error_count = 0
        for x, y in zip(self.X, self.y):
            score = np.multiply((np.dot(x, self.weights.T)), y)
            prediction = np.round(self.sigmoid(score))
            if prediction != 1:
                error_count += 1
        accuracy = 1 - error_count * 1.0 / len(self.X)
        print('total error count:{}'.format(error_count))
        print('accuracy:{}'.format(accuracy))

        return accuracy


def plot_logistic(X, y, weights):
    """
        changed from other's code for visualization
        not working for this assignment, as the data is not really fit for logistic regression
    """
    w0, w1, w2 = weights[1:]

    # plot 3d data points
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    dp1, dp2 = [], []
    for dp, cur_y in zip(X, y):
        if cur_y == 1:
            dp1.append(dp)
        else:
            dp2.append(dp)
    dp1 = np.array(dp1)
    dp2 = np.array(dp2)

    # scatter for raw data
    ax.scatter(dp1[:, 0], dp1[:, 1], dp1[:, 2], color='red', marker='^')
    ax.scatter(dp2[:, 0], dp2[:, 1], dp2[:, 2], color='blue', marker='^')

    GX1, GX2 = np.mgrid[-1:1:0.25, -1:1:0.25]
    GX0 = np.ones(GX1.shape)
    Z = np.divide(1, 1 + np.exp(-1 * (np.multiply(w0, GX0) + np.multiply(w1, GX1) + np.multiply(w2, GX2))))
    ax.plot_surface(GX1, GX2, Z, cstride=1, rstride=1, alpha=0.2)
    plt.title('Logistic Regression Algorithm\n3D data points with hyper plane generated by weights')
    plt.show()


def plotter(X, y, weights):
    # plot 3d data points
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    dp1, dp2 = [], []
    for dp, cur_y in zip(X, y):
        if cur_y == 1:
            dp1.append(dp)
        else:
            dp2.append(dp)
    dp1 = np.array(dp1)
    dp2 = np.array(dp2)

    # scatter for raw data
    ax.scatter(dp1[:, 0], dp1[:, 1], dp1[:, 2], color='red', marker='^')
    ax.scatter(dp2[:, 0], dp2[:, 1], dp2[:, 2], color='blue', marker='^')

    # plot the hyper plane by weights
    point = np.array([0, 0, 0])
    normal = np.array(weights[1:])
    d = -point.dot(normal)

    # create x,y
    xx, yy = np.meshgrid(range(2), range(2))
    z = (-normal[0] * xx - normal[1] * yy - d) * 1. / normal[2]
    ax.plot_surface(xx, yy, z, alpha=0.2)

    ax.set_xlabel('X Label')
    ax.set_ylabel('Y Label')
    ax.set_zlabel('Z Label')

    plt.legend()
    plt.title('Perceptron Algorithm\n3D data points with hyper plane generated by weights')
    plt.show()



X, y = get_data()

logistic_reg = LogsiticRegression(X, y)
weights = logistic_reg.fit()
print(weights)
logistic_reg.verify()
plotter(X, y, weights)
# [-0.03150067] [[-0.17769723  0.11445314  0.0767014 ]]

""" 
    below is scikit learn's version of logistic regression
"""
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(fit_intercept=True, C=1e15)
clf.fit(X, y)
print(clf.intercept_, clf.coef_)
print('accuracy:', clf.score(X, y))
